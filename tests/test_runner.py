#!/usr/bin/env python3
"""
Comprehensive test runner for ETL pipeline tests.
Executes all test suites and provides detailed reporting.
"""

import unittest
import sys
import os
import time
from datetime import datetime

# Add the parent directory to the path to import ETL modules
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

def run_all_tests():
    """Run all ETL tests and provide comprehensive reporting."""
    
    # Discover and load all test modules
    test_loader = unittest.TestLoader()
    test_suite = unittest.TestSuite()
    
    # Test modules to run
    test_modules = [
        'test_data_processing',
        'test_database_operations', 
        'test_api_integration',
        'test_fetch_market_data',
        'test_fetch_market_average_data'
    ]
    
    print("ğŸ” Discovering ETL tests...")
    
    for module_name in test_modules:
        try:
            module = __import__(module_name)
            tests = test_loader.loadTestsFromModule(module)
            test_suite.addTests(tests)
            print(f"âœ… Loaded {module_name}: {tests.countTestCases()} tests")
        except ImportError as e:
            print(f"âŒ Failed to load {module_name}: {e}")
        except Exception as e:
            print(f"âš ï¸ Error loading {module_name}: {e}")
    
    print(f"\nğŸ“Š Total tests discovered: {test_suite.countTestCases()}")
    
    # Run tests with detailed output
    print("\nğŸš€ Starting ETL test execution...")
    print("=" * 80)
    
    start_time = time.time()
    
    # Create test runner with detailed output
    test_runner = unittest.TextTestRunner(
        verbosity=2,
        stream=sys.stdout,
        descriptions=True,
        failfast=False
    )
    
    # Run tests
    result = test_runner.run(test_suite)
    
    end_time = time.time()
    execution_time = end_time - start_time
    
    # Generate comprehensive report
    print("\n" + "=" * 80)
    print("ğŸ“‹ ETL TEST EXECUTION REPORT")
    print("=" * 80)
    
    print(f"â±ï¸  Execution Time: {execution_time:.2f} seconds")
    print(f"ğŸ“ˆ Total Tests: {result.testsRun}")
    print(f"âœ… Passed: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"âŒ Failed: {len(result.failures)}")
    print(f"âš ï¸  Errors: {len(result.errors)}")
    print(f"ğŸ“Š Success Rate: {((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100):.1f}%")
    
    # Detailed failure and error reporting
    if result.failures:
        print(f"\nâŒ FAILURES ({len(result.failures)}):")
        for test, traceback in result.failures:
            print(f"  â€¢ {test}: {traceback.split('AssertionError:')[-1].strip()}")
    
    if result.errors:
        print(f"\nâš ï¸  ERRORS ({len(result.errors)}):")
        for test, traceback in result.errors:
            print(f"  â€¢ {test}: {traceback.split('Exception:')[-1].strip()}")
    
    # Test coverage summary
    print(f"\nğŸ“‹ TEST COVERAGE SUMMARY:")
    print(f"  â€¢ Data Processing Tests: âœ… API response parsing, data validation, error handling")
    print(f"  â€¢ Database Operations Tests: âœ… Data insertion accuracy, update operations, batch processing")
    print(f"  â€¢ API Integration Tests: âœ… Rate limiting, retry logic, error handling")
    print(f"  â€¢ Market Data Pipeline: âœ… End-to-end data flow testing")
    
    # Recommendations based on results
    print(f"\nğŸ’¡ RECOMMENDATIONS:")
    if result.failures or result.errors:
        print(f"  â€¢ Review failed tests and fix underlying issues")
        print(f"  â€¢ Check API endpoint availability and rate limits")
        print(f"  â€¢ Verify database connection and schema")
        print(f"  â€¢ Ensure all environment variables are properly set")
    else:
        print(f"  â€¢ All tests passing! ETL pipeline is ready for production")
        print(f"  â€¢ Consider adding performance benchmarks")
        print(f"  â€¢ Monitor API rate limits in production")
    
    return result.wasSuccessful()

def run_specific_test_category(category):
    """Run tests for a specific category."""
    category_tests = {
        'data_processing': ['test_data_processing'],
        'database': ['test_database_operations'],
        'api': ['test_api_integration'],
        'market_data': ['test_fetch_market_data', 'test_fetch_market_average_data']
    }
    
    if category not in category_tests:
        print(f"âŒ Unknown test category: {category}")
        print(f"Available categories: {list(category_tests.keys())}")
        return False
    
    test_loader = unittest.TestLoader()
    test_suite = unittest.TestSuite()
    
    for module_name in category_tests[category]:
        try:
            module = __import__(module_name)
            tests = test_loader.loadTestsFromModule(module)
            test_suite.addTests(tests)
            print(f"âœ… Loaded {module_name}: {tests.countTestCases()} tests")
        except ImportError as e:
            print(f"âŒ Failed to load {module_name}: {e}")
            return False
    
    print(f"\nğŸš€ Running {category} tests...")
    test_runner = unittest.TextTestRunner(verbosity=2)
    result = test_runner.run(test_suite)
    
    return result.wasSuccessful()

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='ETL Test Runner')
    parser.add_argument('--category', choices=['data_processing', 'database', 'api', 'market_data', 'all'],
                       default='all', help='Test category to run')
    
    args = parser.parse_args()
    
    if args.category == 'all':
        success = run_all_tests()
    else:
        success = run_specific_test_category(args.category)
    
    sys.exit(0 if success else 1) 